{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Библиотеки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:38:54.804506800Z",
     "start_time": "2023-11-09T02:38:52.006489500Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy \n",
    "import pygame\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Создание среды "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:36:37.055835200Z",
     "start_time": "2023-11-09T02:36:37.039996900Z"
    }
   },
   "outputs": [],
   "source": [
    "enviroment_name='CartPole-v1' \n",
    "# даем имя среде \n",
    "#среда с палкой \n",
    "# у нас есть палка закрепленная на плоской поверхности, которая может двигаться только вдоль одной оси . \n",
    "# позиция тележки, скорость , угол \n",
    "# Палка может двигаться только вправо и влево вдоль оси \n",
    "# Награда выдается за каждый шаг, при условии , что палка остается в вертикальном положении "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:36:37.921858400Z",
     "start_time": "2023-11-09T02:36:37.905834900Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:36:38.698329300Z",
     "start_time": "2023-11-09T02:36:38.666569200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space #метод предоставляет информацию о пространстве среды "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:36:39.759078300Z",
     "start_time": "2023-11-09T02:36:39.743398800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n      dtype=float32)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high # атрибуты верхних границ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:36:40.409349700Z",
     "start_time": "2023-11-09T02:36:40.393223700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Discrete(2)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space  #влево и вправо , 0 и 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:36:41.168140400Z",
     "start_time": "2023-11-09T02:36:41.152173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "500"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps # максимальное количество шагов в эпизоде "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:36:41.824031500Z",
     "start_time": "2023-11-09T02:36:41.820275400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "475.0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold # порог награды для успешного завершения эпизода "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:36:57.543108800Z",
     "start_time": "2023-11-09T02:36:42.357142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akylbek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "episodes = 5 # 5 эпизодов , итераций , который агент будет проходить \n",
    "timeSteps = 100 #даем количество временных шагов \n",
    "\n",
    "# Цикл для выполнения заданного числа эпизодов\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()  # Начальное состояние среды, для сброса в начало\n",
    "    print(episode)  # Вывод номера текущего эпизода\n",
    "    env.render()  # Визуализация текущего состояния среды\n",
    "\n",
    "    appendedObservation = []  # Список для сохранения последовательности наблюдений\n",
    "\n",
    "    # Цикл для выполнения заданного числа шагов в каждом эпизоде\n",
    "    for timeStep in range(timeSteps):\n",
    "        print(timeStep)  # Вывод номера текущего шага\n",
    "        random_action = env.action_space.sample()  # Генерация случайного действия из пространства действий среды\n",
    "        observation, reward, terminated, truncated, info = env.step(random_action)  # Выполнение действия в среде\n",
    "# observation - текущее наблюдение после выполнения действия в среде.\n",
    "# reward - вознаграждение, полученное за выполненное действие.\n",
    "# terminated - булевое значение, которое указывает, завершен ли эпизод после выполнения действия.\n",
    "# truncated - булевое значение, которое указывает, было ли действие обрезано из-за достижения максимального числа шагов.\n",
    "# info - дополнительная информация о выполненном действии и его последствиях.\n",
    "\n",
    "        appendedObservation.append(observation)  # Добавление текущего наблюдения в список\n",
    "\n",
    "        time.sleep(0.1)  # Пауза для визуализации каждого шага\n",
    "\n",
    "        if terminated:  # Если эпизод завершен\n",
    "            time.sleep(1)  # Пауза перед началом следующего эпизода\n",
    "            break  # Прерывание цикла\n",
    "\n",
    "env.close()  # Закрытие среды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train an RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:39:00.641502700Z",
     "start_time": "2023-11-09T02:39:00.275072500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akylbek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\") \n",
    "env = DummyVecEnv([lambda: env]) #Векторизации среды . Оптимизирует процесс обучения, дает возмонжность обрабатывать сразу несколько сред\n",
    "model = PPO('MlpPolicy', env, verbose = 1)# тип политики MLpPolicy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T02:37:25.555680200Z",
     "start_time": "2023-11-09T02:37:04.048994700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akylbek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20000\u001B[39;49m\u001B[43m)\u001B[49m \n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# начнет процесс обучения модели PPO на заданном количестве временных шагов.  Наша модель будет обучаться в течении \"total_timesteps=20000\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001B[0m, in \u001B[0;36mPPO.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    300\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfPPO,\n\u001B[0;32m    301\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    306\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    307\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfPPO:\n\u001B[1;32m--> 308\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    309\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:259\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 259\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m continue_training \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m    262\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:178\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space, spaces\u001B[38;5;241m.\u001B[39mBox):\n\u001B[0;32m    176\u001B[0m     clipped_actions \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mclip(actions, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mlow, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mhigh)\n\u001B[1;32m--> 178\u001B[0m new_obs, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclipped_actions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mnum_envs\n\u001B[0;32m    182\u001B[0m \u001B[38;5;66;03m# Give access to local variables\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001B[0m, in \u001B[0;36mVecEnv.step\u001B[1;34m(self, actions)\u001B[0m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;124;03mStep the environments with the given action\u001B[39;00m\n\u001B[0;32m    192\u001B[0m \n\u001B[0;32m    193\u001B[0m \u001B[38;5;124;03m:param actions: the action\u001B[39;00m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;124;03m:return: observation, reward, done, information\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_async(actions)\n\u001B[1;32m--> 197\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001B[0m, in \u001B[0;36mDummyVecEnv.step_wait\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvStepReturn:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;66;03m# Avoid circular imports\u001B[39;00m\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[1;32m---> 58\u001B[0m         obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_rews[env_idx], terminated, truncated, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_infos[env_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m         \u001B[38;5;66;03m# convert to SB3 VecEnv api\u001B[39;00m\n\u001B[0;32m     62\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_dones[env_idx] \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shimmy\\openai_gym_compatibility.py:117\u001B[0m, in \u001B[0;36mGymV26CompatibilityV0.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: ActType) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[ObsType, \u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m]:\n\u001B[0;32m    109\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment.\u001B[39;00m\n\u001B[0;32m    110\u001B[0m \n\u001B[0;32m    111\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgym_env\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[0;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \n\u001B[0;32m     42\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m \n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 50\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:187\u001B[0m, in \u001B[0;36mCartPoleEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    184\u001B[0m     reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 187\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32), reward, terminated, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:298\u001B[0m, in \u001B[0;36mCartPoleEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    297\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[1;32m--> 298\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    299\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mflip()\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000) \n",
    "# начнет процесс обучения модели PPO на заданном количестве временных шагов.  Наша модель будет обучаться в течении \"total_timesteps=20000\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Сохранение моделки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:39:09.821147800Z",
     "start_time": "2023-11-09T02:39:09.816083600Z"
    }
   },
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:39:10.391094200Z",
     "start_time": "2023-11-09T02:39:10.381572600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:39:19.581867200Z",
     "start_time": "2023-11-09T02:39:19.581356Z"
    }
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:39:59.438924200Z",
     "start_time": "2023-11-09T02:39:59.390972300Z"
    }
   },
   "outputs": [],
   "source": [
    "model = PPO.load('Training/Saved Models/PPO_model', env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Оценочная политика эффективности обученного агента "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:40:02.957527700Z",
     "start_time": "2023-11-09T02:40:02.941597Z"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:40:08.523527500Z",
     "start_time": "2023-11-09T02:40:03.945245Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akylbek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akylbek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Тестирование "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:40:18.010284800Z",
     "start_time": "2023-11-09T02:40:17.925122400Z"
    }
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m obs \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m      3\u001B[0m     action, _states \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(obs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:76\u001B[0m, in \u001B[0;36mDummyVecEnv.reset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvObs:\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[1;32m---> 76\u001B[0m         obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset_infos[env_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_seeds\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_obs(env_idx, obs)\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;66;03m# Seeds are only used once\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shimmy\\openai_gym_compatibility.py:106\u001B[0m, in \u001B[0;36mGymV26CompatibilityV0.reset\u001B[1;34m(self, seed, options)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mreset(seed\u001B[38;5;241m=\u001B[39mseed)\n\u001B[0;32m    105\u001B[0m \u001B[38;5;66;03m# Options are ignored\u001B[39;00m\n\u001B[1;32m--> 106\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgym_env\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:68\u001B[0m, in \u001B[0;36mTimeLimit.reset\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \n\u001B[0;32m     61\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03m    The reset environment\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 68\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:42\u001B[0m, in \u001B[0;36mOrderEnforcing.reset\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 42\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:47\u001B[0m, in \u001B[0;36mPassiveEnvChecker.reset\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_reset_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:206\u001B[0m, in \u001B[0;36mCartPoleEnv.reset\u001B[1;34m(self, seed, options)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps_beyond_terminated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 206\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32), {}\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:295\u001B[0m, in \u001B[0;36mCartPoleEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    292\u001B[0m gfxdraw\u001B[38;5;241m.\u001B[39mhline(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msurf, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscreen_width, carty, (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msurf \u001B[38;5;241m=\u001B[39m pygame\u001B[38;5;241m.\u001B[39mtransform\u001B[38;5;241m.\u001B[39mflip(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msurf, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 295\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscreen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msurf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    297\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n",
      "\u001B[1;31merror\u001B[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render() \n",
    "    if done: \n",
    "        print('info', info)\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:40:22.734614500Z",
     "start_time": "2023-11-09T02:40:22.718800900Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. просмотр "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:41:02.047062300Z",
     "start_time": "2023-11-09T02:41:02.031228200Z"
    }
   },
   "outputs": [],
   "source": [
    "log_path = 'Training/Saved Models/PPO_model'\n",
    "training_log_path = os.path.join(log_path, 'PPO_3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:41:04.728405300Z",
     "start_time": "2023-11-09T02:41:04.697132100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"tensorboard\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Добавление обратного вызова к этапу обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:41:06.907925200Z",
     "start_time": "2023-11-09T02:41:06.902654400Z"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:41:08.017841900Z",
     "start_time": "2023-11-09T02:41:08.001735Z"
    }
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:25:17.232769500Z",
     "start_time": "2023-11-09T02:25:17.090252200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'environment_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[43menvironment_name\u001B[49m)\n\u001B[0;32m      2\u001B[0m env \u001B[38;5;241m=\u001B[39m DummyVecEnv([\u001B[38;5;28;01mlambda\u001B[39;00m: env])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'environment_name' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.108504100Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=190, verbose=1)\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=10000, \n",
    "                             best_model_save_path=save_path, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.122113600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing shimmy installation. You provided an OpenAI Gym environment. Stable-Baselines3 (SB3) has transitioned to using Gymnasium internally. In order to use OpenAI Gym environments with SB3, you need to install shimmy (`pip install 'shimmy>=0.2.1'`).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:40\u001B[0m, in \u001B[0;36m_patch_env\u001B[1;34m(env)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 40\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshimmy\u001B[39;00m  \u001B[38;5;66;03m# pytype: disable=import-error\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'shimmy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mPPO\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMlpPolicy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:104\u001B[0m, in \u001B[0;36mPPO.__init__\u001B[1;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     79\u001B[0m     policy: Union[\u001B[38;5;28mstr\u001B[39m, Type[ActorCriticPolicy]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    102\u001B[0m     _init_setup_model: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    103\u001B[0m ):\n\u001B[1;32m--> 104\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    105\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    106\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgae_lambda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgae_lambda\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43ment_coef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ment_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvf_coef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvf_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_grad_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_grad_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_sde\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_sde\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[43msde_sample_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msde_sample_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstats_window_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstats_window_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensorboard_log\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_init_setup_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupported_action_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBox\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDiscrete\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMultiDiscrete\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMultiBinary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;66;03m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001B[39;00m\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;66;03m# because of the advantage normalization\u001B[39;00m\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m normalize_advantage:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:81\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.__init__\u001B[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     60\u001B[0m     policy: Union[\u001B[38;5;28mstr\u001B[39m, Type[ActorCriticPolicy]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     79\u001B[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001B[38;5;241m.\u001B[39mSpace], \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     80\u001B[0m ):\n\u001B[1;32m---> 81\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m     82\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     84\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     87\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_sde\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_sde\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[43m        \u001B[49m\u001B[43msde_sample_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msde_sample_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupport_multi_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstats_window_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstats_window_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     93\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensorboard_log\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupported_action_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msupported_action_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_steps \u001B[38;5;241m=\u001B[39m n_steps\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;241m=\u001B[39m gamma\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:169\u001B[0m, in \u001B[0;36mBaseAlgorithm.__init__\u001B[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m env \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    168\u001B[0m     env \u001B[38;5;241m=\u001B[39m maybe_make_env(env, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n\u001B[1;32m--> 169\u001B[0m     env \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_env\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitor_wrapper\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mobservation_space\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39maction_space\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:216\u001B[0m, in \u001B[0;36mBaseAlgorithm._wrap_env\u001B[1;34m(env, verbose, monitor_wrapper)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" \"\u001B[39;00m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;124;03mWrap environment with the appropriate wrappers if needed.\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03mFor instance, to have a vectorized environment\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;124;03m:return: The wrapped environment.\u001B[39;00m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(env, VecEnv):\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# Patch to support gym 0.21/0.26 and gymnasium\u001B[39;00m\n\u001B[1;32m--> 216\u001B[0m     env \u001B[38;5;241m=\u001B[39m \u001B[43m_patch_env\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_wrapped(env, Monitor) \u001B[38;5;129;01mand\u001B[39;00m monitor_wrapper:\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m verbose \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:42\u001B[0m, in \u001B[0;36m_patch_env\u001B[1;34m(env)\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshimmy\u001B[39;00m  \u001B[38;5;66;03m# pytype: disable=import-error\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing shimmy installation. You provided an OpenAI Gym environment. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     44\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStable-Baselines3 (SB3) has transitioned to using Gymnasium internally. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     45\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use OpenAI Gym environments with SB3, you need to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstall shimmy (`pip install \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshimmy>=0.2.1\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     47\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m     49\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou provided an OpenAI Gym environment. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe strongly recommend transitioning to Gymnasium environments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStable-Baselines3 is automatically wrapping your environments in a compatibility \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayer, which could potentially cause issues.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     54\u001B[0m )\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m signature(env\u001B[38;5;241m.\u001B[39munwrapped\u001B[38;5;241m.\u001B[39mreset)\u001B[38;5;241m.\u001B[39mparameters:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;66;03m# Gym 0.26+ env\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: Missing shimmy installation. You provided an OpenAI Gym environment. Stable-Baselines3 (SB3) has transitioned to using Gymnasium internally. In order to use OpenAI Gym environments with SB3, you need to install shimmy (`pip install 'shimmy>=0.2.1'`)."
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.232769500Z"
    }
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.232769500Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Saved Models', 'best_model')\n",
    "model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.232769500Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:25:17.461557500Z",
     "start_time": "2023-11-09T02:25:17.306395Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Изменение политики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:25:17.587902900Z",
     "start_time": "2023-11-09T02:25:17.373009100Z"
    }
   },
   "outputs": [],
   "source": [
    "net_arch=[dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:25:17.660265300Z",
     "start_time": "2023-11-09T02:25:17.390862200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing shimmy installation. You provided an OpenAI Gym environment. Stable-Baselines3 (SB3) has transitioned to using Gymnasium internally. In order to use OpenAI Gym environments with SB3, you need to install shimmy (`pip install 'shimmy>=0.2.1'`).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:40\u001B[0m, in \u001B[0;36m_patch_env\u001B[1;34m(env)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 40\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshimmy\u001B[39;00m  \u001B[38;5;66;03m# pytype: disable=import-error\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'shimmy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mPPO\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMlpPolicy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnet_arch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet_arch\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:104\u001B[0m, in \u001B[0;36mPPO.__init__\u001B[1;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     79\u001B[0m     policy: Union[\u001B[38;5;28mstr\u001B[39m, Type[ActorCriticPolicy]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    102\u001B[0m     _init_setup_model: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    103\u001B[0m ):\n\u001B[1;32m--> 104\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    105\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    106\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgae_lambda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgae_lambda\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43ment_coef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ment_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvf_coef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvf_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_grad_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_grad_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_sde\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_sde\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[43msde_sample_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msde_sample_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstats_window_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstats_window_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensorboard_log\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_init_setup_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupported_action_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBox\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDiscrete\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMultiDiscrete\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMultiBinary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;66;03m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001B[39;00m\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;66;03m# because of the advantage normalization\u001B[39;00m\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m normalize_advantage:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:81\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.__init__\u001B[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     60\u001B[0m     policy: Union[\u001B[38;5;28mstr\u001B[39m, Type[ActorCriticPolicy]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     79\u001B[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001B[38;5;241m.\u001B[39mSpace], \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     80\u001B[0m ):\n\u001B[1;32m---> 81\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m     82\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     84\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     87\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_sde\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_sde\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[43m        \u001B[49m\u001B[43msde_sample_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msde_sample_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupport_multi_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstats_window_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstats_window_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     93\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensorboard_log\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupported_action_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msupported_action_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_steps \u001B[38;5;241m=\u001B[39m n_steps\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;241m=\u001B[39m gamma\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:169\u001B[0m, in \u001B[0;36mBaseAlgorithm.__init__\u001B[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m env \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    168\u001B[0m     env \u001B[38;5;241m=\u001B[39m maybe_make_env(env, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n\u001B[1;32m--> 169\u001B[0m     env \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_env\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitor_wrapper\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mobservation_space\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39maction_space\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:216\u001B[0m, in \u001B[0;36mBaseAlgorithm._wrap_env\u001B[1;34m(env, verbose, monitor_wrapper)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" \"\u001B[39;00m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;124;03mWrap environment with the appropriate wrappers if needed.\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03mFor instance, to have a vectorized environment\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;124;03m:return: The wrapped environment.\u001B[39;00m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(env, VecEnv):\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# Patch to support gym 0.21/0.26 and gymnasium\u001B[39;00m\n\u001B[1;32m--> 216\u001B[0m     env \u001B[38;5;241m=\u001B[39m \u001B[43m_patch_env\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_wrapped(env, Monitor) \u001B[38;5;129;01mand\u001B[39;00m monitor_wrapper:\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m verbose \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:42\u001B[0m, in \u001B[0;36m_patch_env\u001B[1;34m(env)\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshimmy\u001B[39;00m  \u001B[38;5;66;03m# pytype: disable=import-error\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing shimmy installation. You provided an OpenAI Gym environment. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     44\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStable-Baselines3 (SB3) has transitioned to using Gymnasium internally. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     45\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use OpenAI Gym environments with SB3, you need to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstall shimmy (`pip install \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshimmy>=0.2.1\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     47\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m     49\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou provided an OpenAI Gym environment. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe strongly recommend transitioning to Gymnasium environments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStable-Baselines3 is automatically wrapping your environments in a compatibility \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayer, which could potentially cause issues.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     54\u001B[0m )\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m signature(env\u001B[38;5;241m.\u001B[39munwrapped\u001B[38;5;241m.\u001B[39mreset)\u001B[38;5;241m.\u001B[39mparameters:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;66;03m# Gym 0.26+ env\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: Missing shimmy installation. You provided an OpenAI Gym environment. Stable-Baselines3 (SB3) has transitioned to using Gymnasium internally. In order to use OpenAI Gym environments with SB3, you need to install shimmy (`pip install 'shimmy>=0.2.1'`)."
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, policy_kwargs={'net_arch': net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.458204100Z"
    }
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Использование DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:25:17.732059900Z",
     "start_time": "2023-11-09T02:25:17.474085900Z"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.477716500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing shimmy installation. You provided an OpenAI Gym environment. Stable-Baselines3 (SB3) has transitioned to using Gymnasium internally. In order to use OpenAI Gym environments with SB3, you need to install shimmy (`pip install 'shimmy>=0.2.1'`).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:40\u001B[0m, in \u001B[0;36m_patch_env\u001B[1;34m(env)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 40\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshimmy\u001B[39;00m  \u001B[38;5;66;03m# pytype: disable=import-error\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'shimmy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mDQN\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMlpPolicy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:104\u001B[0m, in \u001B[0;36mDQN.__init__\u001B[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     78\u001B[0m     policy: Union[\u001B[38;5;28mstr\u001B[39m, Type[DQNPolicy]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    102\u001B[0m     _init_setup_model: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    103\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 104\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    105\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    106\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_starts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtau\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgradient_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[43maction_noise\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# No action noise\u001B[39;49;00m\n\u001B[0;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreplay_buffer_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreplay_buffer_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreplay_buffer_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreplay_buffer_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstats_window_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstats_window_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensorboard_log\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m        \u001B[49m\u001B[43msde_support\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimize_memory_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimize_memory_usage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupported_action_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDiscrete\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupport_multi_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexploration_initial_eps \u001B[38;5;241m=\u001B[39m exploration_initial_eps\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexploration_final_eps \u001B[38;5;241m=\u001B[39m exploration_final_eps\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:110\u001B[0m, in \u001B[0;36mOffPolicyAlgorithm.__init__\u001B[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, use_sde_at_warmup, sde_support, supported_action_spaces)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     82\u001B[0m     policy: Union[\u001B[38;5;28mstr\u001B[39m, Type[BasePolicy]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    108\u001B[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001B[38;5;241m.\u001B[39mSpace], \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    109\u001B[0m ):\n\u001B[1;32m--> 110\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstats_window_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstats_window_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensorboard_log\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupport_multi_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msupport_multi_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmonitor_wrapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmonitor_wrapper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_sde\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_sde\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m        \u001B[49m\u001B[43msde_sample_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msde_sample_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupported_action_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msupported_action_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer_size \u001B[38;5;241m=\u001B[39m buffer_size\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size \u001B[38;5;241m=\u001B[39m batch_size\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:169\u001B[0m, in \u001B[0;36mBaseAlgorithm.__init__\u001B[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m env \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    168\u001B[0m     env \u001B[38;5;241m=\u001B[39m maybe_make_env(env, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n\u001B[1;32m--> 169\u001B[0m     env \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_env\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitor_wrapper\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mobservation_space\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39maction_space\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:216\u001B[0m, in \u001B[0;36mBaseAlgorithm._wrap_env\u001B[1;34m(env, verbose, monitor_wrapper)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" \"\u001B[39;00m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;124;03mWrap environment with the appropriate wrappers if needed.\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03mFor instance, to have a vectorized environment\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;124;03m:return: The wrapped environment.\u001B[39;00m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(env, VecEnv):\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# Patch to support gym 0.21/0.26 and gymnasium\u001B[39;00m\n\u001B[1;32m--> 216\u001B[0m     env \u001B[38;5;241m=\u001B[39m \u001B[43m_patch_env\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_wrapped(env, Monitor) \u001B[38;5;129;01mand\u001B[39;00m monitor_wrapper:\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m verbose \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:42\u001B[0m, in \u001B[0;36m_patch_env\u001B[1;34m(env)\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshimmy\u001B[39;00m  \u001B[38;5;66;03m# pytype: disable=import-error\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing shimmy installation. You provided an OpenAI Gym environment. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     44\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStable-Baselines3 (SB3) has transitioned to using Gymnasium internally. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     45\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use OpenAI Gym environments with SB3, you need to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstall shimmy (`pip install \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshimmy>=0.2.1\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     47\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m     49\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou provided an OpenAI Gym environment. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe strongly recommend transitioning to Gymnasium environments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStable-Baselines3 is automatically wrapping your environments in a compatibility \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayer, which could potentially cause issues.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     54\u001B[0m )\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m signature(env\u001B[38;5;241m.\u001B[39munwrapped\u001B[38;5;241m.\u001B[39mreset)\u001B[38;5;241m.\u001B[39mparameters:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;66;03m# Gym 0.26+ env\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: Missing shimmy installation. You provided an OpenAI Gym environment. Stable-Baselines3 (SB3) has transitioned to using Gymnasium internally. In order to use OpenAI Gym environments with SB3, you need to install shimmy (`pip install 'shimmy>=0.2.1'`)."
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.563243400Z"
    }
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.563243400Z"
    }
   },
   "outputs": [],
   "source": [
    "dqn_path = os.path.join('Training', 'Saved Models', 'DQN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.587902900Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39msave(dqn_path)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(dqn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-09T02:25:17.620359800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dqn_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m DQN\u001B[38;5;241m.\u001B[39mload(\u001B[43mdqn_path\u001B[49m, env\u001B[38;5;241m=\u001B[39menv)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dqn_path' is not defined"
     ]
    }
   ],
   "source": [
    "model = DQN.load(dqn_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:25:18.324279600Z",
     "start_time": "2023-11-09T02:25:17.660265300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m evaluate_policy(\u001B[43mmodel\u001B[49m, env, n_eval_episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, render\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:25:18.329828600Z",
     "start_time": "2023-11-09T02:25:17.674770400Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
